.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_10_hrf.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_plot_10_hrf.py:


.. _tut-fnirs-hrf:

GLM Analysis (Measured Data)
============================

In this example we analyse data from a real multichannel fNIRS
experiment (see :ref:`tut-fnirs-hrf-sim` for a simplified simulated
analysis). The experiment consists of three conditions
1) tapping on the left hand,
2) tapping on the right hand,
3) a control condition where the participant does nothing.
We use a GLM analysis to examine the neural activity associated with
the different tapping conditions.
An alternative epoching style analysis on the same data can be
viewed in the
`MNE documentation <https://mne.tools/stable/auto_tutorials/preprocessing/plot_70_fnirs_processing.html>`_.

This GLM analysis is a wrapper over the excellent
`Nilearn stats <https://github.com/nilearn/nilearn/tree/master/nilearn/stats>`_.

.. warning::
      This is a work in progress. Comments are appreciated. To provide feedback please create a github issue.

.. contents:: Page contents
   :local:
   :depth: 2


.. code-block:: default



    # Authors: Robert Luke <mail@robertluke.net>
    #
    # License: BSD (3-clause)

    import os
    import matplotlib.pyplot as plt
    import mne
    import mne_nirs
    import numpy as np

    from mne_nirs.experimental_design import make_first_level_design_matrix
    from mne_nirs.statistics import run_GLM
    from mne_nirs.visualisation import plot_glm_topo

    from nilearn.reporting import plot_design_matrix
    from mne_nirs.channels import get_long_channels, get_short_channels
    from mne_nirs.utils._io import glm_to_tidy, _tidy_long_to_wide









Import raw NIRS data
--------------------

First we import the motor tapping data, these data are also
described and used in the
`MNE fNIRS tutorial <https://mne.tools/stable/auto_tutorials/preprocessing/plot_70_fnirs_processing.html>`_.

After reading the data we resample down to 1Hz
to meet github memory constraints.

.. collapse:: Data description (click to expand)
   :class: success

   Optodes were placed over the motor cortex using the standard NIRX motor
   montage, but with 8 short channels added (see their web page for details).
   To view the sensor locations run
   `raw_intensity.plot_sensors()`.
   A sound was presented to indicate which hand the participant should tap.
   Participants taped their thumb to fingers for 5s.
   Conditions were presented in a random order with a randomised inter
   stimulus interval.


.. code-block:: default


    fnirs_data_folder = mne.datasets.fnirs_motor.data_path()
    fnirs_raw_dir = os.path.join(fnirs_data_folder, 'Participant-1')
    raw_intensity = mne.io.read_raw_nirx(fnirs_raw_dir).load_data()
    raw_intensity.resample(1.0)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Using default location ~/mne_data for fnirs_motor...
    Creating ~/mne_data
    Downloading archive MNE-fNIRS-motor-data.tgz to /github/home/mne_data
    Downloading https://files.osf.io/v1/resources/rxvq7/providers/osfstorage/5dbf84a9cfc96c000ec957eb?version=1&action=download&direct (17.1 MB)
      0%|          | Downloading : 0.00/17.1M [00:00<?,        ?B/s]      6%|5         | Downloading : 0.99M/17.1M [00:00<00:00,    33.8MB/s]     15%|#4        | Downloading : 2.49M/17.1M [00:00<00:00,    34.8MB/s]     26%|##6       | Downloading : 4.49M/17.1M [00:00<00:00,    35.7MB/s]     44%|####3     | Downloading : 7.49M/17.1M [00:00<00:00,    36.9MB/s]     56%|#####5    | Downloading : 9.49M/17.1M [00:00<00:00,    37.6MB/s]     67%|######7   | Downloading : 11.5M/17.1M [00:00<00:00,    38.0MB/s]     79%|#######9  | Downloading : 13.5M/17.1M [00:00<00:00,    39.0MB/s]     91%|######### | Downloading : 15.5M/17.1M [00:00<00:00,    40.1MB/s]    100%|##########| Downloading : 17.1M/17.1M [00:00<00:00,    41.2MB/s]    100%|##########| Downloading : 17.1M/17.1M [00:00<00:00,    68.5MB/s]
    Verifying hash c4935d19ddab35422a69f3326a01fef8.
    Decompressing the archive: /github/home/mne_data/MNE-fNIRS-motor-data.tgz
    (please be patient, this can take some time)
    Successfully extracted to: ['/github/home/mne_data/MNE-fNIRS-motor-data']
    Attempting to create new mne-python configuration file:
    /github/home/.mne/mne-python.json
    Loading /github/home/mne_data/MNE-fNIRS-motor-data/Participant-1
    Reading 0 ... 23238  =      0.000 ...  2974.464 secs...

    <RawNIRX | Participant-1, 56 x 2975 (2974.0 s), ~1.4 MB, data loaded>



Clean up annotations before analysis
------------------------------------

Next we update the annotations by assigning names to each trigger ID.
Then we crop the recording to the section containing our
experimental conditions.


.. code-block:: default


    new_des = [des for des in raw_intensity.annotations.description]
    new_des = ['Control' if x == "1.0" else x for x in new_des]
    new_des = ['Tapping/Left' if x == "2.0" else x for x in new_des]
    new_des = ['Tapping/Right' if x == "3.0" else x for x in new_des]
    annot = mne.Annotations(raw_intensity.annotations.onset,
                            raw_intensity.annotations.duration, new_des)
    raw_intensity.set_annotations(annot)
    raw_intensity.annotations.crop(35, 2967)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <Annotations | 90 segments: Control (30), Tapping/Left (30), Tapping/Right ...>



Preprocess NIRS data
--------------------
Next we convert the raw data to haemoglobin concentration.


.. code-block:: default


    raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)
    raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od)









.. sidebar:: Relevant literature

   Tachtsidis, Ilias, and Felix Scholkmann. "False positives and false
   negatives in functional near-infrared spectroscopy: issues, challenges,
   and the way forward." Neurophotonics 3.3 (2016): 031405.

We then split the data in to
short channels which predominantly contain systemic responses and
long channels which have both neural and systemic contributions.


.. code-block:: default


    short_chs = get_short_channels(raw_haemo)
    raw_haemo = get_long_channels(raw_haemo)









View experiment events
----------------------

Next we examine the timing and order of events in this experiment.
There are several options for how to view event information.
The first option is to use MNE's plot events command.
Here each dot represents when an event started.
We observe that the order of conditions was randomised and the time between
events is also randomised.


.. code-block:: default


    events, _ = mne.events_from_annotations(raw_haemo, verbose=False)
    event_dict = {'Control': 1, 'Tapping/Left': 2, 'Tapping/Right': 3}
    mne.viz.plot_events(events, event_id=event_dict,
                        sfreq=raw_haemo.info['sfreq'])





.. image:: /auto_examples/images/sphx_glr_plot_10_hrf_001.png
    :alt: plot 10 hrf
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <Figure size 640x480 with 1 Axes>



The previous plot did not illustrate the duration that an event lasted for.
Alternatively, we can view the experiment using a boxcar plot, where the
line is raised for the duration of the stimulus/condition.


.. code-block:: default


    s = mne_nirs.experimental_design.create_boxcar(raw_haemo)
    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15, 6))
    plt.plot(raw_haemo.times, s, axes=axes)
    plt.legend(["Control", "Left", "Right"], loc="upper right")
    plt.xlabel("Time (s)");





.. image:: /auto_examples/images/sphx_glr_plot_10_hrf_002.png
    :alt: plot 10 hrf
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Used Annotations descriptions: ['Control', 'Tapping/Left', 'Tapping/Right']

    Text(0.5, 0, 'Time (s)')



Create design matrix
--------------------

.. sidebar:: Relevant literature

   For further discussion on design matrices see
   the NILearn examples. Specifically the 
   `first level model <https://5712-1235740-gh.circle-artifacts.com/0/doc/_build/html/auto_examples/plot_first_level_model_details.html>`_
   and 
   `design matrix examples <https://5712-1235740-gh.circle-artifacts.com/0/doc/_build/html/auto_examples/04_glm_first_level_models/plot_design_matrix.html>`_.

Next we create a model to fit our data to.
The model consists of various components to model different things we assume
contribute to the measured signal.
We model the expected neural response for each experimental condition
using the SPM haemodynamic response
function combined with the known stimulus event times and durations
(as described above).
We also include a third order polynomial drift and constant to model
slow fluctuations in the data and a constant DC shift.


.. code-block:: default


    design_matrix = make_first_level_design_matrix(raw_intensity,
                                                   hrf_model='spm', stim_dur=5.0,
                                                   drift_order=3,
                                                   drift_model='polynomial')









We also add the mean of the short channels to the design matrix.
In theory these channels contain only systemic components, so including
them in the design matrix allows us to estimate the neural component
related to each experimental condition
uncontaminated by systemic effects.


.. code-block:: default


    design_matrix["ShortHbO"] = np.mean(short_chs.copy().pick(
                                        picks="hbo").get_data(), axis=0)

    design_matrix["ShortHbR"] = np.mean(short_chs.copy().pick(
                                        picks="hbr").get_data(), axis=0)









And we display a summary of the design matrix
using standard Nilearn reporting functions.
The first three columns represent the SPM HRF convolved with our stimulus
event information.
The next columns illustrate the drift and constant components.
The last columns illustrate the short channel signals.


.. code-block:: default


    fig, ax1 = plt.subplots(figsize=(10, 6), nrows=1, ncols=1)
    fig = plot_design_matrix(design_matrix, ax=ax1)





.. image:: /auto_examples/images/sphx_glr_plot_10_hrf_003.png
    :alt: plot 10 hrf
    :class: sphx-glr-single-img





Examine expected response
-------------------------

The matrices above can be a bit abstract as they encompase multiple 
conditons and regressors.
Instead we can examine a single condition.
Here we observe the boxcar function for a single condition,
this illustrates when the stimulus was active.
We also view the expected neural response using the HRF specified above,
we observe that each time a stimulus is presented there is an expected
brain response that lags the stimulus onset and consists of a large positive
component followed by an undershoot.


.. code-block:: default


    s = mne_nirs.experimental_design.create_boxcar(raw_intensity)
    plt.plot(raw_intensity.times, s[:, 1])
    plt.plot(design_matrix['Tapping/Left'])
    plt.xlim(180, 300)
    plt.legend(["Stimulus", "Expected Response"])
    plt.xlabel("Time (s)")
    plt.ylabel("Amplitude")





.. image:: /auto_examples/images/sphx_glr_plot_10_hrf_004.png
    :alt: plot 10 hrf
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Used Annotations descriptions: ['Control', 'Tapping/Left', 'Tapping/Right']

    Text(0, 0.5, 'Amplitude')



Fit GLM to subset of data and estimate response for each experimental condition
-------------------------------------------------------------------------------

.. sidebar:: Relevant literature

   Huppert TJ. Commentary on the statistical properties of noise and its
   implication on general linear models in functional near-infrared
   spectroscopy. Neurophotonics. 2016;3(1)

We run a GLM fit for the data and experiment matrix.
First we analyse just the first two channels which correspond HbO and HbR
of a single source detector pair.


.. code-block:: default


    data_subset = raw_haemo.copy().pick(picks=range(2))
    glm_est = run_GLM(data_subset, design_matrix)








We then display the results. Note that the control condition sits
around zero.
And that the HbO is positive and larger than the HbR, this is to be expected.
Further, we note that for this channel the response to tapping on the
right hand is larger than the left. And the values are similar to what
is seen in the epoching tutorial.


.. code-block:: default


    plt.scatter(design_matrix.columns[:3], glm_est['S1_D1 hbo'].theta[:3] * 1e6)
    plt.scatter(design_matrix.columns[:3], glm_est['S1_D1 hbr'].theta[:3] * 1e6)
    plt.xlabel("Experiment Condition")
    plt.ylabel("Haemoglobin (μM)")
    plt.legend(["Oxyhaemoglobin", "Deoxyhaemoglobin"])
    plt.hlines([0.0], 0, 2)





.. image:: /auto_examples/images/sphx_glr_plot_10_hrf_005.png
    :alt: plot 10 hrf
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.collections.LineCollection object at 0x7f7b295e81f0>



Fit GLM to all data and view topographic distribution
-----------------------------------------------------

Lastly we can run the GLM analysis on all sensors and plot the result on a
topomap.
We see the same result as in the MNE tutorial,
that activation is largest
contralateral to the tapping side. Also note that HbR tends to be the
negative sof HbO as expected.


.. code-block:: default


    glm_est = run_GLM(raw_haemo, design_matrix)
    plot_glm_topo(raw_haemo, glm_est, design_matrix,
                  requested_conditions=['Tapping/Left',
                                        'Tapping/Right'])





.. image:: /auto_examples/images/sphx_glr_plot_10_hrf_006.png
    :alt: Tapping/Left, Tapping/Right, Tapping/Left, Tapping/Right
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <Figure size 1200x700 with 6 Axes>



Compute contrasts
-----------------

We can also define a contrast as described in
`Nilearn docs <https://5874-1235740-gh.circle-artifacts.com/0/doc/_build/html/auto_examples/04_glm_first_level_models/plot_localizer_surface_analysis.html>`_
and plot it.
Here we contrast the response to tapping on the left hand with the response
from tapping on the right hand.


.. code-block:: default


    contrast_matrix = np.eye(design_matrix.shape[1])
    basic_conts = dict([(column, contrast_matrix[i])
                       for i, column in enumerate(design_matrix.columns)])
    contrast_LvR = basic_conts['Tapping/Right'] - basic_conts['Tapping/Left']
    contrast = mne_nirs.statistics.compute_contrast(glm_est, contrast_LvR)
    mne_nirs.visualisation.plot_glm_contrast_topo(raw_haemo, contrast)





.. image:: /auto_examples/images/sphx_glr_plot_10_hrf_007.png
    :alt: hbo, hbr
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <Figure size 1200x700 with 3 Axes>



Export Results
---------------

.. warning::
      The functions used in this section are in development, and are highly
      likely to change. These functions are marked with an underscore (_)
      at the start of their name to indicate they are not public functions
      and have no promise they will be here next week.

.. sidebar:: Relevant literature

   Wickham, Hadley. "Tidy data." Journal of Statistical Software 59.10 (2014): 1-23.

Here we export the data in a tidy pandas data frame. Data is exported in
long format by default.
However, a helper function is also provided to convert the long data to wide format.
The long to wide conversion also adds some additonal derived data, such as
if a significant response (p<0.05) was observed, which sensor and detector is
in the channel, which chroma, etc.


.. code-block:: default



    df = glm_to_tidy(raw_haemo, glm_est, design_matrix)
    df = _tidy_long_to_wide(df)









Determine true and false positive rates
---------------------------------------

We can query the exported data frames to determine the true and false
positive rates. Note: optodes cover a greater region than just the
motor cortex, so we dont expect 100% of channels to detect responses to
the tapping, but we do expect 5% or less for the false positive rate.


.. code-block:: default


    (df
     .query('condition in ["Control", "Tapping/Left", "Tapping/Right"]')
     .groupby(['condition', 'Chroma'])
     .agg(['mean'])
     .drop(['df', 'mse', 'p_value', 't'], 1)
     )





.. only:: builder_html

    .. raw:: html

        <div>
        <style scoped>
            .dataframe tbody tr th:only-of-type {
                vertical-align: middle;
            }

            .dataframe tbody tr th {
                vertical-align: top;
            }

            .dataframe thead tr th {
                text-align: left;
            }

            .dataframe thead tr:last-of-type th {
                text-align: right;
            }
        </style>
        <table border="1" class="dataframe">
          <thead>
            <tr>
              <th></th>
              <th></th>
              <th>theta</th>
              <th>Significant</th>
            </tr>
            <tr>
              <th></th>
              <th></th>
              <th>mean</th>
              <th>mean</th>
            </tr>
            <tr>
              <th>condition</th>
              <th>Chroma</th>
              <th></th>
              <th></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th rowspan="2" valign="top">Control</th>
              <th>hbo</th>
              <td>-9.066196e-08</td>
              <td>0.00</td>
            </tr>
            <tr>
              <th>hbr</th>
              <td>6.750751e-09</td>
              <td>0.00</td>
            </tr>
            <tr>
              <th rowspan="2" valign="top">Tapping/Left</th>
              <th>hbo</th>
              <td>7.249248e-06</td>
              <td>0.85</td>
            </tr>
            <tr>
              <th>hbr</th>
              <td>-3.190265e-06</td>
              <td>0.75</td>
            </tr>
            <tr>
              <th rowspan="2" valign="top">Tapping/Right</th>
              <th>hbo</th>
              <td>8.276313e-06</td>
              <td>0.90</td>
            </tr>
            <tr>
              <th>hbr</th>
              <td>-3.095988e-06</td>
              <td>0.80</td>
            </tr>
          </tbody>
        </table>
        </div>
        <br />
        <br />


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  36.018 seconds)


.. _sphx_glr_download_auto_examples_plot_10_hrf.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_10_hrf.py <plot_10_hrf.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_10_hrf.ipynb <plot_10_hrf.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
